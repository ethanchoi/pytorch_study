{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New York City Taxi Fare Prediction\n",
    "### Can you predict a rider's taxi fare?\n",
    "\n",
    "\n",
    "This is one of the Kaggle Competition for biginner, alredy completed.\n",
    "Even it is very simple and straitforward for estimation, i thought it will be a good example to practice estimation problem from the scrath. \n",
    "\n",
    "You can refer to here for details, https://www.kaggle.com/c/new-york-city-taxi-fare-prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir('./')\n",
    "files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please download pre-created train_s file which samples 10% of the original file randomly\n",
    "**train_s.csv file download** \n",
    "[train_s.csv](https://drive.google.com/uc?export=download&id=1AbO1jfrwJ0IKSJQactC6msPNub131LAh)\n",
    "<br>**test.csv file download**\n",
    "[test_s.csv](https://drive.google.com/uc?export=download&id=1iE_JilybsBIBpaTveD6vX8AfwaZzI8i8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's read csv data into pandas dataframe\n",
    "It will take some time to load as train_s.csv is a bit huge, about 601MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sdf = pd.read_csv('./train_s.csv', index_col=0)\n",
    "test_sdf = pd.read_csv('./test_s.csv', index_col=0)\n",
    "test_df = pd.read_csv('./test.csv', index_col=0)\n",
    "print('The size of train data =', train_sdf.shape)\n",
    "print('The size of test =', test_sdf.shape)\n",
    "print('The size of test for submission =', test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's see what the data looks like**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sdf.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Nan and Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look over train data if there are Nan in any of features\n",
    "print(train_sdf.isnull().sum() , test_sdf.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove missing values\n",
    "train_sdf = train_sdf.dropna(how = 'any')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From [the starter code](https://www.kaggle.com/dster/nyc-taxi-fare-starter-kernel-simple-linear-model) \n",
    "\n",
    "Add columns\n",
    "\n",
    "```\n",
    "# Given a dataframe, add two new features 'abs_diff_longitude' and\n",
    "# 'abs_diff_latitude' reprensenting the \"Manhattan vector\" from\n",
    "# the pickup location to the dropoff location.\n",
    "def add_travel_vector_features(df):\n",
    "    df['abs_diff_longitude'] = (df.dropoff_longitude - df.pickup_longitude).abs()\n",
    "    df['abs_diff_latitude'] = (df.dropoff_latitude - df.pickup_latitude).abs()\n",
    "\n",
    "add_travel_vector_features(train_df)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_travel_vector_features(df):\n",
    "    df['abs_diff_longitude'] = (df.dropoff_longitude - df.pickup_longitude).abs()\n",
    "    df['abs_diff_latitude'] = (df.dropoff_latitude - df.pickup_latitude).abs()\n",
    "\n",
    "# add longitude, latitude difference to column features\n",
    "add_travel_vector_features(train_sdf)\n",
    "add_travel_vector_features(test_sdf)\n",
    "add_travel_vector_features(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering out outliers\n",
    "From the starter code<br>\n",
    "\"We expect most of these values to be very small (likely between 0 and 1) since it should all be differences between GPS coordinates within one city. For reference, one degree of latitude is about 69 miles. However, we can see the dataset has extreme values which do not make sense. Let's remove those values from our training set. Based on the scatterplot, it looks like we can safely exclude values above 5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sdf = train_sdf[(train_sdf.abs_diff_longitude < 5.0) & (train_sdf.abs_diff_latitude < 5.0)]\n",
    "print(train_sdf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removing locata data out of region of test set data**<br>\n",
    "Find out longitude, latitude min, max of test data, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def location_boundary(df):\n",
    "    \"\"\"\n",
    "    Find min, max values of logitude, latitude in df\n",
    "    \n",
    "    Return\n",
    "        (longitude_min, longitude_max), (latitude_min, latitude_max)\n",
    "    \"\"\"\n",
    "    \n",
    "    log_min = min(df.pickup_longitude.min(), df.dropoff_longitude.min())\n",
    "    log_max = min(df.pickup_longitude.max(), df.dropoff_longitude.max())\n",
    "    \n",
    "    lat_min = min(df.pickup_latitude.min(), df.dropoff_latitude.min())\n",
    "    lat_max = min(df.pickup_latitude.max(), df.dropoff_latitude.max())\n",
    "    \n",
    "    return (log_min, log_max), (lat_min, lat_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(lon_min, lon_max), (lat_min, lat_max) = location_boundary(test_df)\n",
    "print(lon_min, lon_max, lat_min, lat_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only taken data which is within test_s set location boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sdf = \\\n",
    "    train_sdf[(train_sdf.pickup_longitude >= lon_min) & (train_sdf.pickup_longitude <= lon_max) &\n",
    "    (train_sdf.dropoff_longitude >= lon_min) & (train_sdf.dropoff_longitude <= lon_max) &\n",
    "    (train_sdf.pickup_latitude >= lat_min) & (train_sdf.pickup_latitude <= lat_max) &\n",
    "    (train_sdf.dropoff_latitude >= lat_min) & (train_sdf.dropoff_latitude <= lat_max)]\n",
    "\n",
    "print(train_sdf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Most of cases, passenger_count is lower than 6, let's regard as outliers if it is more than 6, remove 0 passenger count as well**\n",
    "\n",
    "Already applied to test_s set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.groupby('passenger_count').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sdf = train_sdf[(train_sdf.passenger_count <= 6) & (train_sdf.passenger_count > 0)]\n",
    "print(train_sdf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's remove fare_amount lower than 2.5**<br>\n",
    "Because, the basic fare is 2.5, most of case is under $100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sdf = train_sdf[ (train_sdf.fare_amount >= 2.5) & (train_sdf.fare_amount < 100)]\n",
    "print(train_sdf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add pickup time infomation as input features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sdf['pickup_datetime'] = pd.to_datetime(train_sdf['pickup_datetime'],format='%Y-%m-%d %H:%M:%S UTC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickup_time_featues(data):\n",
    "    \"\"\"\n",
    "    Insert year, month, day, day of week, hour, minute information \n",
    "    from pickup_datatime column\n",
    "    \"\"\"\n",
    "    \n",
    "    data['Year'] = data['pickup_datetime'].dt.year\n",
    "    data['Month'] = data['pickup_datetime'].dt.month\n",
    "    data['Date'] = data['pickup_datetime'].dt.day\n",
    "    data['DayofWeek'] = data['pickup_datetime'].dt.dayofweek\n",
    "    data['Hour'] = data['pickup_datetime'].dt.hour\n",
    "    data['Minute'] = data['pickup_datetime'].dt.minute\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sdf = pickup_time_featues(train_sdf)\n",
    "train_sdf.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sdf['pickup_datetime'] = pd.to_datetime(test_sdf['pickup_datetime'],format='%Y-%m-%d %H:%M:%S UTC')\n",
    "test_df['pickup_datetime'] = pd.to_datetime(test_df['pickup_datetime'],format='%Y-%m-%d %H:%M:%S UTC')\n",
    "test_sdf = pickup_time_featues(test_sdf)\n",
    "test_df = pickup_time_featues(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's see the final size of data again now\n",
    "print('The shape of train data =', train_sdf.shape)\n",
    "print('The shape of test =', test_sdf.shape)\n",
    "print('The size of test for submission =', test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Columns to drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df, cols_to_drop='', label='fare_amount'):\n",
    "    \"\"\"\n",
    "    Returns df_x(features), df_y(labels)\n",
    "    \"\"\"\n",
    "    drop_cols = []\n",
    "    for col in cols_to_drop:\n",
    "        if col in df:\n",
    "            drop_cols.append(col)\n",
    "\n",
    "    df_x = df.drop(columns=drop_cols)\n",
    "\n",
    "    if label is not None:\n",
    "        df_y = df_x.pop(label)\n",
    "        return df_x, df_y\n",
    "    else:\n",
    "        return df_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sdf.columns.values\n",
    "train_sdf.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns to drop in input features\n",
    "# simply not to use pickup_datetime as a feature for estimation here.\n",
    "COLUMNS_TO_DROP = ['pickup_datetime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = preprocess_data(train_sdf, COLUMNS_TO_DROP, 'fare_amount')\n",
    "test_x, test_y = preprocess_data(test_sdf, COLUMNS_TO_DROP, 'fare_amount')\n",
    "stest_x = preprocess_data(test_df, COLUMNS_TO_DROP, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_x.shape, train_y.shape, test_x.shape, test_y.shape, stest_x.shape)\n",
    "train_x.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _MinMax(dframes):\n",
    "    \"\"\"\n",
    "    Find out min max values of each columns among data frame listed in dframes\n",
    "    \"\"\"\n",
    "    s_min = dframes[0].min(axis=0)\n",
    "    s_max = dframes[0].max(axis=0)\n",
    "    \n",
    "    for df in dframes[1:]:\n",
    "        d_min =  df.min(axis=0)\n",
    "        d_max =  df.max(axis=0)\n",
    "        s_min = s_min.combine(d_min, min)\n",
    "        s_max = s_max.combine(d_max, max)\n",
    "    return s_min, s_max\n",
    "\n",
    "# panda series type\n",
    "x_min, x_max = _MinMax([train_x, test_x, stest_x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's implement your own pytorch code here\n",
    "\n",
    "prepprocess - drop columns which will not be used as feature columns <br>\n",
    "Define Torch Dataset\n",
    "    ```Class NewYorkTaxiFareDataset(Dataset) ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df, cols_to_drop=''):\n",
    "    \"\"\"\n",
    "    Returns df with colums dropped\n",
    "    \"\"\"\n",
    "    drop_cols = []\n",
    "    for col in cols_to_drop:\n",
    "        if col in df:\n",
    "            drop_cols.append(col)\n",
    "    df = df.drop(columns=drop_cols)\n",
    "    return df\n",
    "    \n",
    "class NewYorkTaxiFareDataset(Dataset):\n",
    "    \"\"\"New York Taxi Fare dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, df, drop_cols, label=None, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df : data frame which is read from csv file\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.label = label\n",
    "        self.drop_cols = drop_cols\n",
    "        self.df = preprocess(self.df, self.drop_cols)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "    \n",
    "        if self.label is not None:\n",
    "            taxi_fare_tg = np.array([self.df.iloc[idx, 0]])\n",
    "            taxi_fare_input = np.array([self.df.iloc[idx, 1:]])\n",
    "            sample = {'features': taxi_fare_input, 'target': taxi_fare_tg}\n",
    "        else:\n",
    "            taxi_fare_input = np.array([self.df.iloc[idx, :]])\n",
    "            sample = {'features': taxi_fare_input}\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _MinMaxScaler(X, s_min, s_max, feature_range=(0, 1)):\n",
    "    \n",
    "    min, max = feature_range\n",
    "    \n",
    "    episilon = 1e-10\n",
    "    X_std = (X - s_min) / (s_max - s_min + episilon)\n",
    "    X_scaled = X_std * (max - min) + min\n",
    "    \n",
    "    return X_scaled\n",
    "\n",
    "class MinMaxScaler(object):\n",
    "    \"\"\"Apply min max scaler to sample\"\"\"\n",
    "    def __init__(self, label, _min, _max):\n",
    "        self.label = label\n",
    "        self.min = _min.to_numpy()\n",
    "        self.max = _max.to_numpy()\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        if self.label is not None:\n",
    "            features, target = sample['features'], sample['target']\n",
    "            features = _MinMaxScaler(features, self.min, self.max)\n",
    "            return {'features' : features, 'target' : target}\n",
    "        else:\n",
    "            features = sample['features']\n",
    "            features = _MinMaxScaler(features, self.min, self.max)\n",
    "            return {'features' : features}\n",
    "    \n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "    def __init__(self, label):\n",
    "        self.label = label\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        if self.label is not None:\n",
    "            taxi_features, taxi_fare = sample['features'], sample['target']\n",
    "            return {'features': torch.from_numpy(taxi_features),\n",
    "                    'target': torch.from_numpy(taxi_fare)}\n",
    "        else:\n",
    "            taxi_features = sample['features']\n",
    "            return {'features': torch.from_numpy(taxi_features)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying transform and get train dataset using data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float64)\n",
    "label = 'fare_amount'\n",
    "train_dataset = NewYorkTaxiFareDataset(train_sdf, COLUMNS_TO_DROP, label=label, \n",
    "                                       transform = transforms.Compose([MinMaxScaler(label,x_min,x_max),\n",
    "                                                                       ToTensor(label)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see transformed dataset, which is tensor, check the size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = train_dataset[0]\n",
    "print('dataset features size=', dataset['features'].shape, '\\n', dataset['features'])\n",
    "print('dataset taxi_fare size=', dataset['target'].shape, '\\n', dataset['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=128,\n",
    "                                          shuffle=True, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(trainloader)\n",
    "print(dataiter.next()['features'].size(), dataiter.next()['target'].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Your Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_NewYorkTaxiFare(nn.Module):\n",
    "    def __init__(self, input_size, output_size, fc_units):\n",
    "        \"\"\"\n",
    "        Model define\n",
    "            input_size - input features size\n",
    "            output_size - final output size\n",
    "            fc_units - list of units of hidden layers\n",
    "        \"\"\"\n",
    "        super(Net_NewYorkTaxiFare, self).__init__()\n",
    "        \n",
    "        self.nn_layers = nn.ModuleList()\n",
    "        \n",
    "        for i in range(len(fc_units)):\n",
    "            if i == 0:\n",
    "                self.nn_layers.append(nn.Linear(input_size, fc_units[i], bias=True))\n",
    "            else:\n",
    "                self.nn_layers.append(nn.Linear(fc_units[i-1], fc_units[i], bias=True))\n",
    "        \n",
    "        self.output = nn.Linear(fc_units[-1], output_size, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Feed Forward\n",
    "        for layer in self.nn_layers:\n",
    "            x = F.relu(layer(x))\n",
    "        x = self.output(x)\n",
    "        x = x.view(-1,1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# two hidden layers with units 13, 13\n",
    "model = Net_NewYorkTaxiFare(13, 1, [13,13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Loss, metrics, optimizer and learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import time\n",
    "\n",
    "metrics = torch.nn.L1Loss()\n",
    "criterion = torch.nn.MSELoss(reduction='mean')\n",
    "learning_rate = 1e-3\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "num_batches = len(trainloader)\n",
    "running_loss = 0.0\n",
    "running_mae = 0.0\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    run_time = time.time()\n",
    "    print('Epoch {:d}/{:d}'.format(epoch,epochs))\n",
    "    \n",
    "    for i, data in enumerate(trainloader):\n",
    "        # time for batch start\n",
    "        batch_st = time.time()\n",
    "        \n",
    "        inputs = data['features']\n",
    "        labels = data['target']\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # print statistics\n",
    "        mae = metrics(outputs, labels)\n",
    "        batch_et = (time.time() - batch_st)\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        running_mae += mae.item()\n",
    "        sys.stdout.write('[%d/%d] %.2fms/step - loss: %.4f - mae: %.4f\\r' % \\\n",
    "                         (i, num_batches, batch_et*1000, loss.item(), mae.item()))\n",
    "        sys.stdout.flush()\n",
    "            \n",
    "    \n",
    "    # Measure one epoch training time\n",
    "    t_epoch = time.time() - run_time\n",
    "    print('[%d/%d] %.2fs %.2fms/step - loss: %.4f - mae: %.4f\\r' % \\\n",
    "                         (i, num_batches, t_epoch, batch_et*1000, \\\n",
    "                          running_loss/num_batches, running_mae/num_batches))\n",
    "    running_loss = 0\n",
    "    running_mae = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get test_s data set tensor for model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test loader for test data set\n",
    "test_dataset = NewYorkTaxiFareDataset(test_sdf, COLUMNS_TO_DROP, label=label, \n",
    "                                       transform = transforms.Compose([MinMaxScaler(label,x_min,x_max),\n",
    "                                                                       ToTensor(label)]))\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=128,\n",
    "                                          shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(testloader)\n",
    "print(dataiter.next()['features'].size(), dataiter.next()['target'].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict = np.array([])\n",
    "with torch.no_grad():\n",
    "    total_loss, total_mae = 0.0, 0.0\n",
    "    num_test_batches = len(testloader)\n",
    "    for i, data in enumerate(testloader, 0):\n",
    "        inputs = data['features']\n",
    "        labels = data['target']\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        mae = metrics(outputs, labels)\n",
    "\n",
    "        test_predict = np.append(outputs.view(-1,).numpy(), test_predict)\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_mae += mae.item()\n",
    "        sys.stdout.write('[%d/%d] - loss: %.4f - mae: %.4f\\r' % \\\n",
    "                         (i, num_test_batches, loss.item(), mae.item()))\n",
    "        sys.stdout.flush()\n",
    "    \n",
    "    print('\\nAverage loss: %.4f  - mae: %.4f\\r' % (total_loss/num_test_batches, total_mae/num_test_batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict = test_predict.reshape(-1,1)\n",
    "print(test_predict.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_taxi_fare = test_y.values.reshape(-1,1)\n",
    "print(test_taxi_fare.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_fare_array = np.concatenate((test_taxi_fare, test_predict), axis=1)\n",
    "print(taxi_fare_array.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataframe with two columns, one is label, 'fare_amoun' and the other is its prediction values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_fare = pd.DataFrame(taxi_fare_array , columns=['fare_amount', 'prediction'])\n",
    "taxi_fare.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = abs(taxi_fare.fare_amount - taxi_fare.prediction)\n",
    "y.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cumulative distribution function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "n, bins, patches = ax.hist(y, bins=y.size, linewidth=2, density=True, \\\n",
    "            histtype='step', cumulative=True)\n",
    "ax.grid(True)\n",
    "ax.set_title('New York Taxi Fare Prediction Error CDF')\n",
    "ax.set_xlabel('fare amount error')\n",
    "ax.set_ylabel('Likelihood of occurrence')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test loader for test data set\n",
    "stest_dataset = NewYorkTaxiFareDataset(stest_x, COLUMNS_TO_DROP, label=None, \n",
    "                                       transform = transforms.Compose([MinMaxScaler(None,x_min,x_max),\n",
    "                                                                       ToTensor(None)]))\n",
    "\n",
    "stestloader = torch.utils.data.DataLoader(stest_dataset, batch_size=128,\n",
    "                                          shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_fare_predict = np.array([])\n",
    "with torch.no_grad():\n",
    "    for data in stestloader:\n",
    "        inputs = data['features']\n",
    "        outputs = model(inputs)\n",
    "        predict = outputs.view(-1,).numpy()\n",
    "        taxi_fare_predict = np.append(taxi_fare_predict, predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = stest_x.index.values\n",
    "print(key.shape, taxi_fare_predict.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the predictions to a CSV file which we can submit to the competition.\n",
    "submission = pd.DataFrame(\n",
    "    {'key': stest_x.index, 'fare_amount': taxi_fare_predict},\n",
    "    columns = ['key', 'fare_amount'])\n",
    "submission.to_csv('submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
