{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zwBCE43Cv3PH"
   },
   "source": [
    "## New York City Taxi Fare Prediction\n",
    "### Can you predict a rider's taxi fare?\n",
    "<br>\n",
    "<br>\n",
    "This is one of the Kaggle Competition for biginner, alredy completed.<br>\n",
    "Even it is very simple and straitforward for estimation, i thought it will be a good example to practice estimation problem from the scrath. <br>\n",
    "\n",
    "You can refer to here for details, https://www.kaggle.com/c/new-york-city-taxi-fare-prediction <br>\n",
    "This is a tensorflow code I made for your reference,\n",
    "Your task is to create PyTorch code and run your own code, and compare it with other competitions.\n",
    "\n",
    "If you have any questions on this implementaion, email [me](jychoi.ethan@gmail.com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5IoRbCA2n0_V"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "#!pip install -q tensorflow==2.0.0-beta1\n",
    "import sys, os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure GPU is available on your machine\n",
    "from tensorflow.python.client import device_lib\n",
    "print(sys.version)\n",
    "print(\"tf.version=\", tf.__version__)\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-2kBGy_pxn47"
   },
   "source": [
    "## Read data using pandas\n",
    "set path of the csv file containing the heart dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VS4w2LePn9g3"
   },
   "outputs": [],
   "source": [
    "files = os.listdir('./')\n",
    "files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6BXRPD2-xtQ1"
   },
   "source": [
    "### The train file is too large and only 10% of the samples are imported for this training.\n",
    "```\n",
    "# the original size of train data is 55423856, about 55M\n",
    "df = df_train.sample(frac=0.1, replace=True, random_state=1)\n",
    "print('The size of train data =', len(df))\n",
    "\n",
    "df.to_csv('./train_s.csv')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please download pre-created train_s file which samples 10% of the original file randomly and it's test_s file for model evaluation\n",
    "**train_s.csv file download** \n",
    "[train_s.csv](https://drive.google.com/uc?export=download&id=10Xnm58RpMlH2q1a5XwUoWvtbEkDV44Jg)\n",
    "<br>**test_s.csv file download**\n",
    "[test_s.csv](https://drive.google.com/uc?export=download&id=1Ej119VhEqt_IstdXgz3d-K_SDWevxghh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the csv file using pandas.<br>\n",
    "Instead of using original train.csv file, use smaller version, train_s.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_file = './train.csv'\n",
    "train_s_file = './train_s.csv'\n",
    "test_s_file = './test_s.csv'\n",
    "# original test file for submission\n",
    "test_file = './test.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's read csv data into pandas dataframe\n",
    "It will take some time to load as train.csv is a bit huge, please be patient "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UEfJ8TcMpe-2"
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(train_s_file, index_col=0)\n",
    "df_test = pd.read_csv(test_s_file, index_col=0)\n",
    "s_test = pd.read_csv(test_file, index_col=0)\n",
    "print('The size of train data =', df_train.shape)\n",
    "print('The size of test =', df_test.shape)\n",
    "print('The size of test for submission =', s_test.shape)\n",
    "\n",
    "print('\\n', df_train.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's see what the data looks like**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocess\n",
    "- Drop columns not to be used for features, rows which has NaN values\n",
    "- Remove outliers\n",
    "- apply MinMaxScaler if possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look over train data if there are Nan in any of features\n",
    "print(df_train.isnull().sum() , df_test.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove missing values\n",
    "df_train = df_train.dropna(how = 'any')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From [the starter code](https://www.kaggle.com/dster/nyc-taxi-fare-starter-kernel-simple-linear-model) \n",
    "\n",
    "Add columns\n",
    "\n",
    "```\n",
    "# Given a dataframe, add two new features 'abs_diff_longitude' and\n",
    "# 'abs_diff_latitude' reprensenting the \"Manhattan vector\" from\n",
    "# the pickup location to the dropoff location.\n",
    "def add_travel_vector_features(df):\n",
    "    df['abs_diff_longitude'] = (df.dropoff_longitude - df.pickup_longitude).abs()\n",
    "    df['abs_diff_latitude'] = (df.dropoff_latitude - df.pickup_latitude).abs()\n",
    "\n",
    "add_travel_vector_features(train_df)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_travel_vector_features(df):\n",
    "    df['abs_diff_longitude'] = (df.dropoff_longitude - df.pickup_longitude).abs()\n",
    "    df['abs_diff_latitude'] = (df.dropoff_latitude - df.pickup_latitude).abs()\n",
    "\n",
    "# add longitude, latitude difference to column features\n",
    "add_travel_vector_features(df_train)\n",
    "add_travel_vector_features(df_test)\n",
    "add_travel_vector_features(s_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering out outliers\n",
    "From the starter code<br>\n",
    "\"We expect most of these values to be very small (likely between 0 and 1) since it should all be differences between GPS coordinates within one city. For reference, one degree of latitude is about 69 miles. However, we can see the dataset has extreme values which do not make sense. Let's remove those values from our training set. Based on the scatterplot, it looks like we can safely exclude values above 5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train[(df_train.abs_diff_longitude < 5.0) & (df_train.abs_diff_latitude < 5.0)]\n",
    "print(df_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find out min, max values of longitude, latitude in test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def location_boundary(df):\n",
    "    \"\"\"\n",
    "    Find min, max values of logitude, latitude in df\n",
    "    \n",
    "    Return\n",
    "        (longitude_min, longitude_max), (latitude_min, latitude_max)\n",
    "    \"\"\"\n",
    "    \n",
    "    log_min = min(df.pickup_longitude.min(), df.dropoff_longitude.min())\n",
    "    log_max = min(df.pickup_longitude.max(), df.dropoff_longitude.max())\n",
    "    \n",
    "    lat_min = min(df.pickup_latitude.min(), df.dropoff_latitude.min())\n",
    "    lat_max = min(df.pickup_latitude.max(), df.dropoff_latitude.max())\n",
    "    \n",
    "    return (log_min, log_max), (lat_min, lat_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(lon_min, lon_max), (lat_min, lat_max) = location_boundary(s_test)\n",
    "print(lon_min, lon_max, lat_min, lat_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's remove train data out of bounding box of test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = \\\n",
    "    df_train[(df_train.pickup_longitude >= lon_min) & (df_train.pickup_longitude <= lon_max) &\n",
    "    (df_train.dropoff_longitude >= lon_min) & (df_train.dropoff_longitude <= lon_max) &\n",
    "    (df_train.pickup_latitude >= lat_min) & (df_train.pickup_latitude <= lat_max) &\n",
    "    (df_train.dropoff_latitude >= lat_min) & (df_train.dropoff_latitude <= lat_max)]\n",
    "\n",
    "print(df_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Most of cases, passenger_count is lower than 6, let's regard as outliers if it is more than 6, remove 0 passenger count as well**\n",
    "\n",
    "Already applied to test_s set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_test.groupby('passenger_count').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train[(df_train.passenger_count <= 6) & (df_train.passenger_count > 0)]\n",
    "print(df_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's remove fare_amount lower than 2.5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train[ (df_train.fare_amount >= 2.5) & (df_train.fare_amount < 100)]\n",
    "print(df_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add pickup time inforation as input features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['pickup_datetime'] = pd.to_datetime(df_train['pickup_datetime'],format='%Y-%m-%d %H:%M:%S UTC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickup_time_featues(data):\n",
    "    \"\"\"\n",
    "    Insert year, month, day, day of week, hour, minute information \n",
    "    from pickup_datatime column\n",
    "    \"\"\"\n",
    "    \n",
    "    data['Year'] = data['pickup_datetime'].dt.year\n",
    "    data['Month'] = data['pickup_datetime'].dt.month\n",
    "    data['Date'] = data['pickup_datetime'].dt.day\n",
    "    data['DayofWeek'] = data['pickup_datetime'].dt.dayofweek\n",
    "    data['Hour'] = data['pickup_datetime'].dt.hour\n",
    "    data['Minute'] = data['pickup_datetime'].dt.minute\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pickup_time_featues(df_train)\n",
    "df_train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['pickup_datetime'] = pd.to_datetime(df_test['pickup_datetime'],format='%Y-%m-%d %H:%M:%S UTC')\n",
    "s_test['pickup_datetime'] = pd.to_datetime(s_test['pickup_datetime'],format='%Y-%m-%d %H:%M:%S UTC')\n",
    "df_test = pickup_time_featues(df_test)\n",
    "s_test = pickup_time_featues(s_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's see the final size of data again now\n",
    "print('The shape of train data =', df_train.shape)\n",
    "print('The shape of test =', df_test.shape)\n",
    "print('The size of test for submission =', s_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set columns to drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df, cols_to_drop='', label='fare_amount'):\n",
    "    \"\"\"\n",
    "    Returns df_x(features), df_y(labels)\n",
    "    \"\"\"\n",
    "    drop_cols = []\n",
    "    for col in cols_to_drop:\n",
    "        if col in df_train:\n",
    "            drop_cols.append(col)\n",
    "\n",
    "    df_x = df.drop(columns=drop_cols)\n",
    "\n",
    "    if label is not None:\n",
    "        df_y = df_x.pop(label)\n",
    "        return df_x, df_y\n",
    "    else:\n",
    "        return df_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns to drop in input features\n",
    "# simply not to use pickup_datetime as a feature for estimation here.\n",
    "COLUMNS_TO_DROP = ['pickup_datetime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = preprocess_data(df_train, COLUMNS_TO_DROP, 'fare_amount')\n",
    "test_x, test_y = preprocess_data(df_test, COLUMNS_TO_DROP, 'fare_amount')\n",
    "stest_x = preprocess_data(s_test, COLUMNS_TO_DROP, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2wwhILm1ycSp"
   },
   "outputs": [],
   "source": [
    "print(train_x.shape, train_y.shape, test_x.shape, test_y.shape, stest_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's find out min, max values of each columns in train, test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _MinMax(dframes):\n",
    "    \"\"\"\n",
    "    Find out min max values of each columns among data frame listed in dframes\n",
    "    \"\"\"\n",
    "    s_min = dframes[0].min(axis=0)\n",
    "    s_max = dframes[0].max(axis=0)\n",
    "    \n",
    "    for df in dframes[1:]:\n",
    "        d_min =  df.min(axis=0)\n",
    "        d_max =  df.max(axis=0)\n",
    "        s_min = s_min.combine(d_min, min)\n",
    "        s_max = s_max.combine(d_max, max)\n",
    "    return s_min, s_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_min, x_max = _MinMax([train_x, test_x, stest_x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_min, '\\n', x_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _MinMaxScaler(X, s_min, s_max, feature_range=(0, 1)):\n",
    "    \n",
    "    min, max = feature_range\n",
    "    \n",
    "    episilon = 1e-10\n",
    "    X_std = (X - s_min) / (s_max - s_min + episilon)\n",
    "    X_scaled = X_std * (max - min) + min\n",
    "    \n",
    "    return X_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply MinMaxScaler to train and test dataframe\n",
    "(This is optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = _MinMaxScaler(train_x, x_min, x_max)\n",
    "test_x = _MinMaxScaler(test_x, x_min, x_max)\n",
    "stest_x = _MinMaxScaler(stest_x, x_min, x_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data using `tf.data.Dataset`\n",
    "\n",
    "Use tf.data.Dataset.from_tensor_slices to read the values from a pandas dataframe.\n",
    "\n",
    "One of the advantages of using tf.data.Dataset is it allows you to write simple, highly efficient data pipelines. Read the loading data guide to find out more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W6Yc-D3aqyBb"
   },
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((train_x.values, train_y.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "chEnp_Swsf0a"
   },
   "source": [
    "For reading<br>\n",
    "```for feat, targ in dataset.take(2):\n",
    "       print ('Features: {}, Target: {}'.format(feat, targ))```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9XLxRHS10Ylp"
   },
   "source": [
    "Shuffle and batch the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R3dQ-83Ztsgl"
   },
   "outputs": [],
   "source": [
    "train_dataset = dataset.shuffle(len(train_x)).batch(BATCH_SIZE)\n",
    "print(train_dataset)\n",
    "_, input_size = train_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bB9C0XJkyQEk"
   },
   "source": [
    "## Create and train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_compiled_model(hidden_units, activation='relu', lr=1e-3, l2_scale=1e-4):\n",
    "    model = tf.keras.Sequential()\n",
    "    \n",
    "    for units in hidden_units:\n",
    "        model.add(tf.keras.layers.Dense(units, \\\n",
    "                activation=activation, kernel_regularizer=tf.keras.regularizers.l2(l2_scale)))\n",
    "    \n",
    "    model.add(tf.keras.layers.Dense(1, activation='relu', \\\n",
    "                        kernel_regularizer=tf.keras.regularizers.l2(l2_scale)))\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    #optimizer = tf.keras.optimizers.RMSprop(learning_rate=lr, momentum=0.9)\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ybDzNUheqxJw"
   },
   "outputs": [],
   "source": [
    "model = get_compiled_model([input_size, 13, 13], lr=1e-3, l2_scale=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ybDzNUheqxJw"
   },
   "outputs": [],
   "source": [
    "# Just run 2 times as a trial\n",
    "model.fit(train_dataset, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_x.values, test_y.values))\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate how your model is accurate for prediction using mean absolute error**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y_mse, test_y_mae = model.evaluate(test_dataset)\n",
    "print('\\nTest Loss {:.4f}, Test Mean Absolute Error {:.4f}'.format(test_y_mse, test_y_mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y_predict = model.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_y_predict.shape)\n",
    "test_y_predict[:10, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y = test_y.values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_fare_predict = np.concatenate((test_y, test_y_predict), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_fare = pd.DataFrame(taxi_fare_predict, columns=['fare_amount', 'prediction'])\n",
    "taxi_fare.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = abs(taxi_fare.fare_amount - taxi_fare.prediction)\n",
    "y.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "n, bins, patches = ax.hist(y, bins=y.size, linewidth=2, density=True, \\\n",
    "            histtype='step', cumulative=True)\n",
    "ax.grid(True)\n",
    "ax.set_title('New York Taxi Fare Prediction Error CDF')\n",
    "ax.set_xlabel('fare amount error')\n",
    "ax.set_ylabel('Likelihood of occurrence')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model predict for final test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stest_dataset = tf.data.Dataset.from_tensor_slices((stest_x.values))\n",
    "stest_dataset = stest_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfare_predict = model.predict(stest_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('the size of fare_predict=', sfare_predict.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfare_predict = sfare_predict.reshape(-1)\n",
    "key = s_test.index.values\n",
    "print(key.shape, sfare_predict.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the predictions to a CSV file which we can submit to the competition.\n",
    "submission = pd.DataFrame(\n",
    "    {'key': s_test.index, 'fare_amount': sfare_predict},\n",
    "    columns = ['key', 'fare_amount'])\n",
    "submission.to_csv('submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "pandas.ipynb",
   "private_outputs": true,
   "provenance": [
    {
     "file_id": "1XwmbuQRh8Xo93CV_op6Q_ty2TAjGoBjc",
     "timestamp": 1558919304199
    }
   ],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
